{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg-Jz-q8Gyi2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc48b2b2-d19b-45a8-d964-4a163a975b4d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2132, 572, 264, 6319, 323, 13458, 88]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "# To use tokenizers, we import them from the transformers library\n",
        "\n",
        "# There are many available, use the ID of the model you want to use\n",
        "# Qwen \"Qwen/Qwen2-0.5B\"\n",
        "# GPT-2 \"openai-community/gpt2\"\n",
        "# SmolLM \"HuggingFaceTB/SomlLM-135M\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B\")\n",
        "prompt = \"It was a dark and stormy\"\n",
        "input_ids = tokenizer(prompt).input_ids\n",
        "print(input_ids)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the token number for each word\n",
        "\n",
        "for t in input_ids:\n",
        "  print(t, \"\\t:\", tokenizer.decode(t))"
      ],
      "metadata": {
        "id": "BDFcsWdeO5b-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed26e3d-7091-487e-bbb9-80f0257fe69f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2132 \t: It\n",
            "572 \t:  was\n",
            "264 \t:  a\n",
            "6319 \t:  dark\n",
            "323 \t:  and\n",
            "13458 \t:  storm\n",
            "88 \t: y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "for token_id in range(0, 50):\n",
        "  token = tokenizer.decode(token_id)\n",
        "  print(token_id, \"\\t:\", token)\n"
      ],
      "metadata": {
        "id": "Mst_ngBBtPE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "287f2391-7358-40eb-b09a-94c8c299efe7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 \t: !\n",
            "1 \t: \"\n",
            "2 \t: #\n",
            "3 \t: $\n",
            "4 \t: %\n",
            "5 \t: &\n",
            "6 \t: '\n",
            "7 \t: (\n",
            "8 \t: )\n",
            "9 \t: *\n",
            "10 \t: +\n",
            "11 \t: ,\n",
            "12 \t: -\n",
            "13 \t: .\n",
            "14 \t: /\n",
            "15 \t: 0\n",
            "16 \t: 1\n",
            "17 \t: 2\n",
            "18 \t: 3\n",
            "19 \t: 4\n",
            "20 \t: 5\n",
            "21 \t: 6\n",
            "22 \t: 7\n",
            "23 \t: 8\n",
            "24 \t: 9\n",
            "25 \t: :\n",
            "26 \t: ;\n",
            "27 \t: <\n",
            "28 \t: =\n",
            "29 \t: >\n",
            "30 \t: ?\n",
            "31 \t: @\n",
            "32 \t: A\n",
            "33 \t: B\n",
            "34 \t: C\n",
            "35 \t: D\n",
            "36 \t: E\n",
            "37 \t: F\n",
            "38 \t: G\n",
            "39 \t: H\n",
            "40 \t: I\n",
            "41 \t: J\n",
            "42 \t: K\n",
            "43 \t: L\n",
            "44 \t: M\n",
            "45 \t: N\n",
            "46 \t: O\n",
            "47 \t: P\n",
            "48 \t: Q\n",
            "49 \t: R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "text = \"It was a dark and stormy\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits[0, -1]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "top_probs, top_ids = torch.topk(probs, 20)\n",
        "\n",
        "print(\"Top 10 next WORDS:\")\n",
        "words = []\n",
        "count = 0\n",
        "for p, tid in zip(top_probs, top_ids):\n",
        "    token = tokenizer.decode([tid.item()])\n",
        "    if token.startswith(\" \") and token.strip().isalpha():\n",
        "        print(token.strip(), \":\", round(float(p)*100, 2), \"%\")\n",
        "        words.append(token.strip())\n",
        "        count += 1\n",
        "        if count == 10:\n",
        "            break\n",
        "\n",
        "print(\"\\nP+7 word is:\", words[6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba50ead-1e6a-4e32-beac-c6332ed61d7a",
        "collapsed": true,
        "id": "5wZBqiUXNt2v"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 next WORDS:\n",
            "night : 46.18 %\n",
            "day : 23.46 %\n",
            "evening : 5.87 %\n",
            "morning : 4.42 %\n",
            "afternoon : 4.11 %\n",
            "summer : 1.34 %\n",
            "time : 1.33 %\n",
            "winter : 1.22 %\n",
            "weekend : 0.39 %\n",
            "one : 0.34 %\n",
            "\n",
            "P+7 word is: time\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***P+7 technique***"
      ],
      "metadata": {
        "id": "lZQWcxL4eohH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load GPT-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "# The poem\n",
        "poem = \"\"\"One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\"\"\"\n",
        "\n",
        "lines = poem.split(\"\\n\")\n",
        "\n",
        "# Function that returns the k-th most likely next word\n",
        "def p_plus_k(prefix, k):\n",
        "    inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "    logits = model(**inputs).logits[0, -1]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    # Take the most likely tokens\n",
        "    top_ids = torch.topk(probs, 2000).indices\n",
        "\n",
        "    # Keep only tokens that look like real words\n",
        "    words = []\n",
        "    for tid in top_ids:\n",
        "        tok = tokenizer.decode([tid.item()])\n",
        "        w = tok.strip()\n",
        "        if tok.startswith(\" \") and w.replace(\"-\", \"\").replace(\"'\", \"\").isalpha():\n",
        "            words.append(w)\n",
        "        if len(words) >= k:\n",
        "            break\n",
        "\n",
        "    return words[k-1]\n",
        "\n",
        "# the value of k\n",
        "K = 7\n",
        "\n",
        "new_lines = []\n",
        "# Replace the last word of each line using P+k\n",
        "for line in lines:\n",
        "    prefix = line.rsplit(\" \", 1)[0] + \" \"\n",
        "    new_word = p_plus_k(prefix, K)\n",
        "    new_lines.append(prefix + new_word)\n",
        "\n",
        "# Join all the modified lines back into a poem\n",
        "result = \"\\n\".join(new_lines)\n",
        "print(result)\n",
        "\n",
        "# Save the result\n",
        "with open(f\"P_plus_{K}.txt\", \"w\") as f:\n",
        "    f.write(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWr96My-V-y_",
        "outputId": "acb430d6-eb46-4fd0-81d4-dda7e0bad82b",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of for\n",
            "To regard the frost and the Glac\n",
            "Of the pine-trees crusted with and\n",
            "And have been cold a long and\n",
            "To behold the junipers shagged with Moines\n",
            "The spruces rough in the distant Mountains\n",
            "Of the January sun; and not to do\n",
            "Of any misery in the sound of the sounds\n",
            "In the sound of a few tuned\n",
            "Which is the sound of the elevator\n",
            "Full of the same though\n",
            "That is blowing in the same bare of\n",
            "For the listener, who listens in the Pond\n",
            "And, nothing himself, did\n",
            "Nothing that is not there and the nothing that happened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***P+39 technique***"
      ],
      "metadata": {
        "id": "xFF3kywlez6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load GPT-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# The poem\n",
        "poem = \"\"\"One must have a mind of winter\n",
        "To regard the frost and the boughs\n",
        "Of the pine-trees crusted with snow;\n",
        "And have been cold a long time\n",
        "To behold the junipers shagged with ice,\n",
        "The spruces rough in the distant glitter\n",
        "Of the January sun; and not to think\n",
        "Of any misery in the sound of the wind,\n",
        "In the sound of a few leaves,\n",
        "Which is the sound of the land\n",
        "Full of the same wind\n",
        "That is blowing in the same bare place\n",
        "For the listener, who listens in the snow,\n",
        "And, nothing himself, beholds\n",
        "Nothing that is not there and the nothing that is.\"\"\"\n",
        "\n",
        "lines = poem.split(\"\\n\")\n",
        "\n",
        "# Function: get the k-th most likely next word\n",
        "def p_plus_k(prefix, k):\n",
        "    inputs = tokenizer(prefix, return_tensors=\"pt\")\n",
        "    logits = model(**inputs).logits[0, -1]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    # Take the most likely tokens\n",
        "    top_ids = torch.topk(probs, 2000).indices\n",
        "\n",
        "    # Keep only tokens that look like real words\n",
        "    words = []\n",
        "    for tid in top_ids:\n",
        "        tok = tokenizer.decode([tid.item()])\n",
        "        w = tok.strip()\n",
        "        if tok.startswith(\" \") and w.replace(\"-\", \"\").replace(\"'\", \"\").isalpha():\n",
        "            words.append(w)\n",
        "        if len(words) >= k:\n",
        "            break\n",
        "\n",
        "    return words[k-1]\n",
        "\n",
        "# the value of k\n",
        "K = 39\n",
        "\n",
        "new_lines = []\n",
        "# Replace the last word of each line using P+k\n",
        "for line in lines:\n",
        "    prefix = line.rsplit(\" \", 1)[0] + \" \"\n",
        "    new_word = p_plus_k(prefix, K)\n",
        "    new_lines.append(prefix + new_word)\n",
        "\n",
        "# Join all the modified lines back into a poem\n",
        "result = \"\\n\".join(new_lines)\n",
        "print(result)\n",
        "\n",
        "# Save the result\n",
        "with open(f\"P_plus_{K}.txt\", \"w\") as f:\n",
        "    f.write(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu2lgOIYg658",
        "outputId": "4c975768-b965-44be-aceb-a3d0b0dc6f5a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One must have a mind of grind\n",
            "To regard the frost and the zombies\n",
            "Of the pine-trees crusted with exactly\n",
            "And have been cold a long tour\n",
            "To behold the junipers shagged with used\n",
            "The spruces rough in the distant Inner\n",
            "Of the January sun; and not to to\n",
            "Of any misery in the sound of the har\n",
            "In the sound of a few practices\n",
            "Which is the sound of the heard\n",
            "Full of the same Gro\n",
            "That is blowing in the same bare Hancock\n",
            "For the listener, who listens in the ring\n",
            "And, nothing himself, as\n",
            "Nothing that is not there and the nothing that been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QXN6EOgYV-Nr"
      }
    }
  ]
}